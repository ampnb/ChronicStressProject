{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hOl5sDXo_C_C"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ampnb/ChronicStressProject/blob/main/EEG_Stress_CNN_Unchunking_Cleaned_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kkVXPs1iOqI",
        "outputId": "c6570ea5-f33a-4b75-8a83-60f073be22a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.5.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1KZjrJfUrB_k1g8VUsohNpek_hvWNqGcr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AnMbRCkb0E",
        "outputId": "c777ef56-e599-4987-877f-a50ecf02dee1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KZjrJfUrB_k1g8VUsohNpek_hvWNqGcr\n",
            "To: /content/Clean_signal.zip\n",
            "100% 113M/113M [00:00<00:00, 214MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Clean_signal.zip'\n",
        "#!mv '/content/clean_exp16' 'Cleaned_signal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY30dqh8kjSM",
        "outputId": "1495aad7-4385-43d7-a66c-3c03edf2ba6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Clean_signal.zip\n",
            "replace __MACOSX/._Clean_signal? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/._Clean_signal  \n",
            "  inflating: Clean_signal/PSS10 - Sheet1.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._PSS10 - Sheet1.csv  \n",
            "  inflating: Clean_signal/clean_exp08.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp08.csv  \n",
            "  inflating: Clean_signal/clean_exp09.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp09.csv  \n",
            "  inflating: Clean_signal/clean_exp01.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp01.csv  \n",
            "  inflating: Clean_signal/clean_exp15.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp15.csv  \n",
            "  inflating: Clean_signal/clean_exp14.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp14.csv  \n",
            "  inflating: Clean_signal/clean_exp16.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp16.csv  \n",
            "  inflating: Clean_signal/clean_exp02.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp02.csv  \n",
            "  inflating: Clean_signal/clean_exp03.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp03.csv  \n",
            "  inflating: Clean_signal/clean_exp17.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp17.csv  \n",
            "  inflating: Clean_signal/clean_exp13.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp13.csv  \n",
            "  inflating: Clean_signal/clean_exp07.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp07.csv  \n",
            "  inflating: Clean_signal/clean_exp06.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp06.csv  \n",
            "  inflating: Clean_signal/clean_exp12.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp12.csv  \n",
            "  inflating: Clean_signal/clean_exp04.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp04.csv  \n",
            "  inflating: Clean_signal/clean_exp10.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp10.csv  \n",
            "  inflating: Clean_signal/clean_exp11.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp11.csv  \n",
            "  inflating: Clean_signal/clean_exp05.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp05.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip0Qx71RiRak",
        "outputId": "ff0b6c24-ee46-4681-a0f5-2a29791174f6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GxiHrcZiRdm",
        "outputId": "c1cd3e62-e9a4-407d-ee23-e7686590f995"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: components in /usr/local/lib/python3.7/dist-packages (1.2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import mne\n",
        "import pandas as pd\n",
        "pickle.format_version\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms  \n",
        "from torch.utils.data import DataLoader, Dataset  \n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# setting seed so that splitting process and training process can be reproduce\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY4wJP1-iRgP",
        "outputId": "5309169b-d0b1-41f3-b010-7de7db29544c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f30f56739f0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL"
      ],
      "metadata": {
        "id": "7F96WFs9ihQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "main_path = os.getcwd()\n",
        "filename = os.listdir(main_path + '/Clean_signal/')"
      ],
      "metadata": {
        "id": "1aQoZm4jibEr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = ['clean_exp01.csv',\n",
        " 'clean_exp02.csv',\n",
        " 'clean_exp03.csv',\n",
        " 'clean_exp04.csv',\n",
        " 'clean_exp05.csv',\n",
        " 'clean_exp06.csv',\n",
        " 'clean_exp07.csv',\n",
        " 'clean_exp08.csv',\n",
        " 'clean_exp09.csv',\n",
        " 'clean_exp10.csv',\n",
        " 'clean_exp11.csv',\n",
        " 'clean_exp12.csv',\n",
        " 'clean_exp13.csv',\n",
        " 'clean_exp14.csv',\n",
        " 'clean_exp15.csv',\n",
        " 'clean_exp16.csv',\n",
        " 'clean_exp17.csv']"
      ],
      "metadata": {
        "id": "Z49O-2H2k6p5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_file_path = main_path + '/Clean_signal/'\n",
        "X_list = list()\n",
        "for file in filename:\n",
        "    file_path = all_file_path + file\n",
        "    df = pd.read_csv(file_path, index_col=None)\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "    X_list.append(df) "
      ],
      "metadata": {
        "id": "7N78RJfiibHn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "eGs1ADkribKx",
        "outputId": "d6f858cc-3e1a-4bb6-a7c5-1087a0837828"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Fp1           Fp2            F7            F3            F4  \\\n",
              "0  1.558541e-13  1.202787e-12 -1.822815e-12 -6.666149e-13 -1.524659e-14   \n",
              "1 -2.495299e+02 -2.591266e+02 -2.392414e+02 -2.472481e+02 -1.342918e+02   \n",
              "2 -1.022146e+02 -1.071695e+02 -8.913196e+01 -9.320015e+01 -7.188064e+01   \n",
              "3 -1.894707e+02 -1.985657e+02 -1.666088e+02 -1.773347e+02 -1.206077e+02   \n",
              "4 -1.976600e+02 -2.088452e+02 -1.691085e+02 -1.829386e+02 -1.219805e+02   \n",
              "\n",
              "             F8            T3            C3            C4            T4  \\\n",
              "0 -9.656176e-14  1.778769e-13 -7.318365e-13 -3.218725e-14  6.776264e-15   \n",
              "1 -2.469675e+02 -2.368676e+02 -2.375381e+02  8.964020e+01  9.114929e+01   \n",
              "2 -9.669399e+01 -7.360927e+01 -7.934479e+01 -8.483305e+00 -3.966042e+00   \n",
              "3 -1.861381e+02 -1.566157e+02 -1.616160e+02  1.053098e+01  1.359837e+01   \n",
              "4 -1.981792e+02 -1.664133e+02 -1.711082e+02  5.430791e+00  5.540176e+00   \n",
              "\n",
              "             T5            P3            P4            T6            O1  \\\n",
              "0  2.463172e-12 -5.293956e-13  1.679243e-13  2.774456e-12 -3.303428e-13   \n",
              "1  9.105020e+01  7.684951e+01  1.383037e+02 -4.155527e+02 -3.083336e+02   \n",
              "2  6.921319e-02 -5.860820e+00  6.340924e+00 -1.006591e+02 -7.385641e+01   \n",
              "3  2.570865e+01  1.375228e+01  4.319836e+01 -2.383818e+02 -1.751361e+02   \n",
              "4  2.039321e+01  9.522058e+00  4.153111e+01 -2.713544e+02 -1.985949e+02   \n",
              "\n",
              "             O2  \n",
              "0  2.217109e-13  \n",
              "1 -1.145710e+02  \n",
              "2 -3.186257e+01  \n",
              "3 -7.472951e+01  \n",
              "4 -9.082392e+01  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37fe0896-9d5b-49f4-8aae-8860e440f034\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fp1</th>\n",
              "      <th>Fp2</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>T3</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>P3</th>\n",
              "      <th>P4</th>\n",
              "      <th>T6</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.558541e-13</td>\n",
              "      <td>1.202787e-12</td>\n",
              "      <td>-1.822815e-12</td>\n",
              "      <td>-6.666149e-13</td>\n",
              "      <td>-1.524659e-14</td>\n",
              "      <td>-9.656176e-14</td>\n",
              "      <td>1.778769e-13</td>\n",
              "      <td>-7.318365e-13</td>\n",
              "      <td>-3.218725e-14</td>\n",
              "      <td>6.776264e-15</td>\n",
              "      <td>2.463172e-12</td>\n",
              "      <td>-5.293956e-13</td>\n",
              "      <td>1.679243e-13</td>\n",
              "      <td>2.774456e-12</td>\n",
              "      <td>-3.303428e-13</td>\n",
              "      <td>2.217109e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.495299e+02</td>\n",
              "      <td>-2.591266e+02</td>\n",
              "      <td>-2.392414e+02</td>\n",
              "      <td>-2.472481e+02</td>\n",
              "      <td>-1.342918e+02</td>\n",
              "      <td>-2.469675e+02</td>\n",
              "      <td>-2.368676e+02</td>\n",
              "      <td>-2.375381e+02</td>\n",
              "      <td>8.964020e+01</td>\n",
              "      <td>9.114929e+01</td>\n",
              "      <td>9.105020e+01</td>\n",
              "      <td>7.684951e+01</td>\n",
              "      <td>1.383037e+02</td>\n",
              "      <td>-4.155527e+02</td>\n",
              "      <td>-3.083336e+02</td>\n",
              "      <td>-1.145710e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.022146e+02</td>\n",
              "      <td>-1.071695e+02</td>\n",
              "      <td>-8.913196e+01</td>\n",
              "      <td>-9.320015e+01</td>\n",
              "      <td>-7.188064e+01</td>\n",
              "      <td>-9.669399e+01</td>\n",
              "      <td>-7.360927e+01</td>\n",
              "      <td>-7.934479e+01</td>\n",
              "      <td>-8.483305e+00</td>\n",
              "      <td>-3.966042e+00</td>\n",
              "      <td>6.921319e-02</td>\n",
              "      <td>-5.860820e+00</td>\n",
              "      <td>6.340924e+00</td>\n",
              "      <td>-1.006591e+02</td>\n",
              "      <td>-7.385641e+01</td>\n",
              "      <td>-3.186257e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.894707e+02</td>\n",
              "      <td>-1.985657e+02</td>\n",
              "      <td>-1.666088e+02</td>\n",
              "      <td>-1.773347e+02</td>\n",
              "      <td>-1.206077e+02</td>\n",
              "      <td>-1.861381e+02</td>\n",
              "      <td>-1.566157e+02</td>\n",
              "      <td>-1.616160e+02</td>\n",
              "      <td>1.053098e+01</td>\n",
              "      <td>1.359837e+01</td>\n",
              "      <td>2.570865e+01</td>\n",
              "      <td>1.375228e+01</td>\n",
              "      <td>4.319836e+01</td>\n",
              "      <td>-2.383818e+02</td>\n",
              "      <td>-1.751361e+02</td>\n",
              "      <td>-7.472951e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.976600e+02</td>\n",
              "      <td>-2.088452e+02</td>\n",
              "      <td>-1.691085e+02</td>\n",
              "      <td>-1.829386e+02</td>\n",
              "      <td>-1.219805e+02</td>\n",
              "      <td>-1.981792e+02</td>\n",
              "      <td>-1.664133e+02</td>\n",
              "      <td>-1.711082e+02</td>\n",
              "      <td>5.430791e+00</td>\n",
              "      <td>5.540176e+00</td>\n",
              "      <td>2.039321e+01</td>\n",
              "      <td>9.522058e+00</td>\n",
              "      <td>4.153111e+01</td>\n",
              "      <td>-2.713544e+02</td>\n",
              "      <td>-1.985949e+02</td>\n",
              "      <td>-9.082392e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37fe0896-9d5b-49f4-8aae-8860e440f034')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37fe0896-9d5b-49f4-8aae-8860e440f034 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37fe0896-9d5b-49f4-8aae-8860e440f034');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X_list)\n",
        "X.shape \n",
        "#17 people \n",
        "#250 hz * 60 secs * 3 min\n",
        "#16 channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbl_IKDCjbk5",
        "outputId": "04269e03-f0a5-4a21-ff8b-24cd1d50a030"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 45000, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape\n",
        "X = np.transpose(X, (0, 2, 1))\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFU5ZNkkinvr",
        "outputId": "c3933f91-af15-4280-859b-9affdec79be5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 16, 45000)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_path = '/content/Clean_signal/PSS10 - Sheet1.csv'\n",
        "df = pd.read_csv(y_path)\n",
        "y = np.array(df['label'])\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58PB1ZyqlLL9",
        "outputId": "4a704a72-858b-4a73-9f0a-7addbc5aa975"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "GOBGZtryk_k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = False, stratify = None)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI2G9Bbsinyi",
        "outputId": "7fa4a38b-d354-4f85-96d9-4c0803ebd71b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13, 16, 45000) (4, 16, 45000) (13,) (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_test)"
      ],
      "metadata": {
        "id": "qYeicq65in1f"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=999)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FqFv4sni7e",
        "outputId": "6aa68274-af5b-4d5d-da5c-9e7daa0c4a3f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11, 16, 45000) (2, 16, 45000) (11,) (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#scale transform each channel independently\n",
        "scalers = {}\n",
        "for i in range(X_train.shape[2]):\n",
        "    scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i]) \n",
        "\n",
        "for i in range(X_val.shape[2]):\n",
        "    X_val[:, :, i]   = scalers[i].transform(X_val[:, :, i])     \n",
        "    \n",
        "for i in range(X_test.shape[2]):\n",
        "    X_test[:, :, i]  = scalers[i].transform(X_test[:, :, i]) "
      ],
      "metadata": {
        "id": "S8pWHAGeni_f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.mean(), X_val.mean(), X_test.mean())\n",
        "print(X_train.min(), X_val.min(), X_test.min())\n",
        "print(X_train.max(), X_val.max(), X_test.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLOO4OpynjDC",
        "outputId": "e3f5f335-0725-469b-f731-85c14de79ce7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0035532945742628333 -0.005295317732894995 -0.0031302117490915734\n",
            "-1.0 -418.85970347464144 -2797.1708453940587\n",
            "1.0000000000000004 749.4431360181195 2064.2315527719506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Batch size and train,test, val loader"
      ],
      "metadata": {
        "id": "RLKlKqBWno9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train).to(torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train).to(torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val).to(torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val).to(torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test).to(torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test).to(torch.float32)\n",
        "\n",
        "# Cast data to dataloader for more convenience\n",
        "training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "testing_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "validation_set = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_batch_size = 3\n",
        "val_batch_size = len(validation_set)\n",
        "test_batch_size = len(testing_set)\n",
        "\n",
        "train_loader = DataLoader(training_set, train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, val_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(testing_set, test_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Y1enUccbhXGK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, label in train_loader:\n",
        "  print(data)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJ3VJt3huYw",
        "outputId": "e2c2cab4-bde8-4174-d6a4-fbf24b262ead"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2625,  1.0000,  0.5220,  ..., -0.1055,  0.4833, -1.0000],\n",
            "         [-0.1846,  1.0000,  0.8369,  ...,  0.6823,  0.9280,  0.0450],\n",
            "         [-0.6639,  1.0000,  0.5339,  ...,  0.1186,  0.7721,  0.4254],\n",
            "         ...,\n",
            "         [ 0.5534, -0.8758, -1.0000,  ..., -0.7690, -0.9071,  0.5299],\n",
            "         [ 0.6211, -0.7073, -1.0000,  ...,  0.1866, -0.2887,  0.1363],\n",
            "         [-1.0000,  0.4953, -0.4354,  ...,  0.8192,  0.1606, -1.0000]],\n",
            "\n",
            "        [[ 0.2994, -0.1263,  0.7198,  ..., -0.3135, -0.2813, -0.9733],\n",
            "         [-0.0283,  0.9211,  0.8718,  ...,  0.6713,  0.8581, -0.9717],\n",
            "         [-1.0000, -0.0266,  0.6665,  ...,  0.0394,  0.1550, -1.0000],\n",
            "         ...,\n",
            "         [ 1.0000, -0.8192, -0.7463,  ..., -0.9722, -0.9601,  1.0000],\n",
            "         [-1.0000, -0.8281, -0.7549,  ..., -1.0000, -0.8562, -0.8752],\n",
            "         [-0.5561,  0.2080, -0.3849,  ...,  0.4928,  0.0105,  0.7527]],\n",
            "\n",
            "        [[ 0.2968,  0.3130,  0.7292,  ..., -1.0000, -1.0000, -0.7500],\n",
            "         [ 0.2077,  0.9244,  0.8293,  ...,  0.6008,  0.8237, -1.0000],\n",
            "         [-0.0490,  0.7991,  0.7552,  ..., -1.0000, -1.0000,  0.4394],\n",
            "         ...,\n",
            "         [-0.0447, -1.0000, -0.6010,  ..., -0.9183, -0.8995, -1.0000],\n",
            "         [ 1.0000, -1.0000, -0.6260,  ..., -0.3759, -0.5769,  1.0000],\n",
            "         [ 1.0000, -0.0737,  0.0366,  ...,  0.7941,  0.1552,  0.2712]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToOmNlhsPSG6",
        "outputId": "c33f066a-4d9a-4c20-f1db-a373116c7975"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 16, 45000])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVMFMBwAjImF",
        "outputId": "ada34eb1-b2fe-4454-daf3-9cacbcbd64e2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 16, 45000])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HaiKzvgCjc3_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN model"
      ],
      "metadata": {
        "id": "tMV89z27_HmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class eegConv1d(nn.Module):\n",
        "    def __init__(self, input_size = 16, out_size=2):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv1d(input_size, 50, kernel_size = 3)\n",
        "        self.c2 = nn.Conv1d(50, 30, kernel_size = 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.maxpool1d = nn.MaxPool1d(2,2)\n",
        "        self.linear = nn.Linear(30 * 22497, out_size) #taking the last hidden state\n",
        "        \n",
        "    def forward(self, seq):\n",
        "        #convo layer 8 -> 50 -> 30\n",
        "        #seq shape: (11, 50, 45000)\n",
        "        out = self.c1(seq)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        # out shape: (11, 50, 44998)\n",
        "        out = self.maxpool1d(out)\n",
        "        # out shape: (11, 50, 22499)\n",
        "        out = self.c2(out)\n",
        "        out = self.relu(out)\n",
        "        # out shape: (11, 30, 22496)\n",
        "        out = out.reshape(seq.size(0), -1)\n",
        "        #out shape: (30, 30*22496)\n",
        "        out = self.linear(out)\n",
        "        #out shape: (30*22496, 2)\n",
        "        return out"
      ],
      "metadata": {
        "id": "U0bk5XjGjoMk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "TIOLWwwXpMpw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(999999)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = eegConv1d(input_size=16).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Good for finding Likelihood\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ],
      "metadata": {
        "id": "kpdAQllGD1tf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "model.train()\n",
        "\n",
        "train_losses = []\n",
        "train_accs   = []\n",
        "valid_losses = []\n",
        "valid_accs   = []\n",
        "\n",
        "\n",
        "#print(f\"Training {type(model).__name__}\")\n",
        "\n",
        "for i in range(epochs):\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    val_total   = 0  \n",
        "    val_correct = 0\n",
        "    train_acc   = 0\n",
        "    val_acc     = 0\n",
        "    \n",
        "    for X_train, y_train in train_loader:\n",
        "    \n",
        "        start_time = time.time()\n",
        "        \n",
        "        X_train = X_train.float().to(device)\n",
        "        y_train = y_train.type(torch.LongTensor).to(device)\n",
        "\n",
        "        #print(X_train.shape, X_train.dtype)\n",
        "\n",
        "        yhat_train = model(X_train)\n",
        "        \n",
        "        #train acc\n",
        "        _, predicted = torch.max(yhat_train.data, 1)  #returns max value, indices\n",
        "        train_total += y_train.size(0)  #keep track of total\n",
        "        train_correct += (predicted == y_train).sum().item()  #.item() give the raw number\n",
        "        train_acc = 100 * (train_correct / train_total)\n",
        "        \n",
        "        #print(y_train.shape, y_train.dtype)\n",
        "        \n",
        "        train_loss = criterion(yhat_train, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "\n",
        "        #val accuracy\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val = X_val.float().to(device)\n",
        "            y_val = y_val.type(torch.LongTensor).to(device)\n",
        "            yhat_val  = model(X_val)\n",
        "            val_loss     = criterion(yhat_val, y_val)\n",
        "            _, predicted = torch.max(yhat_val.data, 1)  #returns max value, indices\n",
        "            val_total += y_val.size(0)  #keep track of total\n",
        "            val_correct += (predicted == y_val).sum().item()  #.item() give the raw number\n",
        "            val_acc = 100 * (val_correct / val_total)\n",
        "\n",
        "        #save the best model\n",
        "        if val_loss < best_valid_loss:\n",
        "            best_valid_loss = val_loss\n",
        "            #print(\"Model:{} saved.\".format(type(model).__name__))\n",
        "            torch.save(model.state_dict(), './models/CNN1D.pt')\n",
        "            best_model_index = i\n",
        "\n",
        "          #for plotting\n",
        "        train_losses.append(train_loss.item())\n",
        "        train_accs  .append(train_acc)\n",
        "        valid_losses.append(val_loss.item())\n",
        "        valid_accs  .append(val_acc)\n",
        "\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f\"Epoch: {i:2.0f} | Train acc: {train_acc: 2.2f} | \" +\n",
        "          f\"loss: {train_loss:2.5f} | Val acc: {val_acc: 2.2f} | \" +\n",
        "          f\"loss: {val_loss:2.5f} | Time: {epoch_mins}m {epoch_secs}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhkyoUI3D1wo",
        "outputId": "01e2975d-371a-437b-c93a-af3671c47390"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | Train acc:  36.36 | loss: 3.69840 | Val acc:  25.00 | loss: 1.61567 | Time: 0m 0s\n",
            "Epoch:  1 | Train acc:  81.82 | loss: 0.00311 | Val acc:  50.00 | loss: 1.96556 | Time: 0m 0s\n",
            "Epoch:  2 | Train acc:  100.00 | loss: 0.00477 | Val acc:  50.00 | loss: 0.98617 | Time: 0m 0s\n",
            "Epoch:  3 | Train acc:  100.00 | loss: 0.00222 | Val acc:  75.00 | loss: 0.42651 | Time: 0m 0s\n",
            "Epoch:  4 | Train acc:  100.00 | loss: 0.02361 | Val acc:  62.50 | loss: 0.57270 | Time: 0m 0s\n",
            "Epoch:  5 | Train acc:  100.00 | loss: 0.01159 | Val acc:  50.00 | loss: 1.16851 | Time: 0m 0s\n",
            "Epoch:  6 | Train acc:  100.00 | loss: 0.00217 | Val acc:  50.00 | loss: 1.57096 | Time: 0m 0s\n",
            "Epoch:  7 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.77415 | Time: 0m 0s\n",
            "Epoch:  8 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.70203 | Time: 0m 0s\n",
            "Epoch:  9 | Train acc:  100.00 | loss: 0.00108 | Val acc:  50.00 | loss: 1.61968 | Time: 0m 0s\n",
            "Epoch: 10 | Train acc:  100.00 | loss: 0.00099 | Val acc:  50.00 | loss: 1.44745 | Time: 0m 0s\n",
            "Epoch: 11 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.34682 | Time: 0m 0s\n",
            "Epoch: 12 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.24919 | Time: 0m 0s\n",
            "Epoch: 13 | Train acc:  100.00 | loss: 0.00027 | Val acc:  50.00 | loss: 1.26059 | Time: 0m 0s\n",
            "Epoch: 14 | Train acc:  100.00 | loss: 0.00506 | Val acc:  50.00 | loss: 1.01745 | Time: 0m 0s\n",
            "Epoch: 15 | Train acc:  100.00 | loss: 0.00017 | Val acc:  50.00 | loss: 1.12106 | Time: 0m 0s\n",
            "Epoch: 16 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 0.95973 | Time: 0m 0s\n",
            "Epoch: 17 | Train acc:  100.00 | loss: 0.00168 | Val acc:  50.00 | loss: 1.02016 | Time: 0m 0s\n",
            "Epoch: 18 | Train acc:  100.00 | loss: 0.00022 | Val acc:  50.00 | loss: 1.09113 | Time: 0m 0s\n",
            "Epoch: 19 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.15495 | Time: 0m 0s\n",
            "Epoch: 20 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.06390 | Time: 0m 0s\n",
            "Epoch: 21 | Train acc:  100.00 | loss: 0.00192 | Val acc:  50.00 | loss: 1.06779 | Time: 0m 0s\n",
            "Epoch: 22 | Train acc:  100.00 | loss: 0.00091 | Val acc:  50.00 | loss: 1.18457 | Time: 0m 0s\n",
            "Epoch: 23 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.14626 | Time: 0m 0s\n",
            "Epoch: 24 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.08565 | Time: 0m 0s\n",
            "Epoch: 25 | Train acc:  100.00 | loss: 0.00118 | Val acc:  50.00 | loss: 1.14705 | Time: 0m 0s\n",
            "Epoch: 26 | Train acc:  100.00 | loss: 0.00017 | Val acc:  50.00 | loss: 1.13032 | Time: 0m 0s\n",
            "Epoch: 27 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.10592 | Time: 0m 0s\n",
            "Epoch: 28 | Train acc:  100.00 | loss: 0.00102 | Val acc:  50.00 | loss: 1.28188 | Time: 0m 0s\n",
            "Epoch: 29 | Train acc:  100.00 | loss: 0.00050 | Val acc:  50.00 | loss: 1.21929 | Time: 0m 0s\n",
            "Epoch: 30 | Train acc:  100.00 | loss: 0.00101 | Val acc:  50.00 | loss: 1.15527 | Time: 0m 0s\n",
            "Epoch: 31 | Train acc:  100.00 | loss: 0.00090 | Val acc:  50.00 | loss: 1.11747 | Time: 0m 0s\n",
            "Epoch: 32 | Train acc:  100.00 | loss: 0.00070 | Val acc:  50.00 | loss: 1.19864 | Time: 0m 0s\n",
            "Epoch: 33 | Train acc:  100.00 | loss: 0.00045 | Val acc:  50.00 | loss: 1.23788 | Time: 0m 0s\n",
            "Epoch: 34 | Train acc:  100.00 | loss: 0.00094 | Val acc:  50.00 | loss: 1.16194 | Time: 0m 0s\n",
            "Epoch: 35 | Train acc:  100.00 | loss: 0.00176 | Val acc:  50.00 | loss: 1.15493 | Time: 0m 0s\n",
            "Epoch: 36 | Train acc:  100.00 | loss: 0.00112 | Val acc:  50.00 | loss: 1.20872 | Time: 0m 0s\n",
            "Epoch: 37 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.19973 | Time: 0m 0s\n",
            "Epoch: 38 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.16461 | Time: 0m 0s\n",
            "Epoch: 39 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.24690 | Time: 0m 0s\n",
            "Epoch: 40 | Train acc:  100.00 | loss: 0.00019 | Val acc:  50.00 | loss: 1.19941 | Time: 0m 0s\n",
            "Epoch: 41 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.11141 | Time: 0m 0s\n",
            "Epoch: 42 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.22178 | Time: 0m 0s\n",
            "Epoch: 43 | Train acc:  100.00 | loss: 0.00098 | Val acc:  50.00 | loss: 1.17905 | Time: 0m 0s\n",
            "Epoch: 44 | Train acc:  100.00 | loss: 0.00176 | Val acc:  50.00 | loss: 1.10027 | Time: 0m 0s\n",
            "Epoch: 45 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.24520 | Time: 0m 0s\n",
            "Epoch: 46 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.18191 | Time: 0m 0s\n",
            "Epoch: 47 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.14717 | Time: 0m 0s\n",
            "Epoch: 48 | Train acc:  100.00 | loss: 0.00060 | Val acc:  50.00 | loss: 1.21565 | Time: 0m 0s\n",
            "Epoch: 49 | Train acc:  100.00 | loss: 0.00033 | Val acc:  50.00 | loss: 1.10350 | Time: 0m 0s\n",
            "Epoch: 50 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.24465 | Time: 0m 0s\n",
            "Epoch: 51 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.18581 | Time: 0m 0s\n",
            "Epoch: 52 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.35291 | Time: 0m 0s\n",
            "Epoch: 53 | Train acc:  100.00 | loss: 0.00092 | Val acc:  50.00 | loss: 1.23444 | Time: 0m 0s\n",
            "Epoch: 54 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.27834 | Time: 0m 0s\n",
            "Epoch: 55 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.34846 | Time: 0m 0s\n",
            "Epoch: 56 | Train acc:  100.00 | loss: 0.00028 | Val acc:  50.00 | loss: 1.22978 | Time: 0m 0s\n",
            "Epoch: 57 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 1.30994 | Time: 0m 0s\n",
            "Epoch: 58 | Train acc:  100.00 | loss: 0.00035 | Val acc:  50.00 | loss: 1.18672 | Time: 0m 0s\n",
            "Epoch: 59 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.13790 | Time: 0m 0s\n",
            "Epoch: 60 | Train acc:  100.00 | loss: 0.00045 | Val acc:  50.00 | loss: 1.27068 | Time: 0m 0s\n",
            "Epoch: 61 | Train acc:  100.00 | loss: 0.00007 | Val acc:  50.00 | loss: 1.22090 | Time: 0m 0s\n",
            "Epoch: 62 | Train acc:  100.00 | loss: 0.00036 | Val acc:  50.00 | loss: 1.40267 | Time: 0m 0s\n",
            "Epoch: 63 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.22275 | Time: 0m 0s\n",
            "Epoch: 64 | Train acc:  100.00 | loss: 0.00062 | Val acc:  50.00 | loss: 1.26032 | Time: 0m 0s\n",
            "Epoch: 65 | Train acc:  100.00 | loss: 0.00078 | Val acc:  50.00 | loss: 1.19278 | Time: 0m 0s\n",
            "Epoch: 66 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.31366 | Time: 0m 0s\n",
            "Epoch: 67 | Train acc:  100.00 | loss: 0.00031 | Val acc:  50.00 | loss: 1.36424 | Time: 0m 0s\n",
            "Epoch: 68 | Train acc:  100.00 | loss: 0.00035 | Val acc:  50.00 | loss: 1.19491 | Time: 0m 0s\n",
            "Epoch: 69 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.26259 | Time: 0m 0s\n",
            "Epoch: 70 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 1.21510 | Time: 0m 0s\n",
            "Epoch: 71 | Train acc:  100.00 | loss: 0.00033 | Val acc:  50.00 | loss: 1.25298 | Time: 0m 0s\n",
            "Epoch: 72 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.27958 | Time: 0m 0s\n",
            "Epoch: 73 | Train acc:  100.00 | loss: 0.00057 | Val acc:  50.00 | loss: 1.22455 | Time: 0m 0s\n",
            "Epoch: 74 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.44526 | Time: 0m 0s\n",
            "Epoch: 75 | Train acc:  100.00 | loss: 0.00022 | Val acc:  50.00 | loss: 1.23228 | Time: 0m 0s\n",
            "Epoch: 76 | Train acc:  100.00 | loss: 0.00018 | Val acc:  50.00 | loss: 1.33154 | Time: 0m 0s\n",
            "Epoch: 77 | Train acc:  100.00 | loss: 0.00033 | Val acc:  50.00 | loss: 1.34150 | Time: 0m 0s\n",
            "Epoch: 78 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.23411 | Time: 0m 0s\n",
            "Epoch: 79 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.28416 | Time: 0m 0s\n",
            "Epoch: 80 | Train acc:  100.00 | loss: 0.00032 | Val acc:  50.00 | loss: 1.21118 | Time: 0m 0s\n",
            "Epoch: 81 | Train acc:  100.00 | loss: 0.00012 | Val acc:  50.00 | loss: 1.32663 | Time: 0m 0s\n",
            "Epoch: 82 | Train acc:  100.00 | loss: 0.00007 | Val acc:  50.00 | loss: 1.22730 | Time: 0m 0s\n",
            "Epoch: 83 | Train acc:  100.00 | loss: 0.00025 | Val acc:  50.00 | loss: 1.22392 | Time: 0m 0s\n",
            "Epoch: 84 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.22173 | Time: 0m 0s\n",
            "Epoch: 85 | Train acc:  100.00 | loss: 0.00012 | Val acc:  50.00 | loss: 1.37881 | Time: 0m 0s\n",
            "Epoch: 86 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.28380 | Time: 0m 0s\n",
            "Epoch: 87 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.35574 | Time: 0m 0s\n",
            "Epoch: 88 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.30566 | Time: 0m 0s\n",
            "Epoch: 89 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.24961 | Time: 0m 0s\n",
            "Epoch: 90 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.34954 | Time: 0m 0s\n",
            "Epoch: 91 | Train acc:  100.00 | loss: 0.00040 | Val acc:  50.00 | loss: 1.32254 | Time: 0m 0s\n",
            "Epoch: 92 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.34686 | Time: 0m 0s\n",
            "Epoch: 93 | Train acc:  100.00 | loss: 0.00024 | Val acc:  50.00 | loss: 1.39053 | Time: 0m 0s\n",
            "Epoch: 94 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.29779 | Time: 0m 0s\n",
            "Epoch: 95 | Train acc:  100.00 | loss: 0.00013 | Val acc:  50.00 | loss: 1.32464 | Time: 0m 0s\n",
            "Epoch: 96 | Train acc:  100.00 | loss: 0.00040 | Val acc:  50.00 | loss: 1.41852 | Time: 0m 0s\n",
            "Epoch: 97 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.30288 | Time: 0m 0s\n",
            "Epoch: 98 | Train acc:  100.00 | loss: 0.00050 | Val acc:  50.00 | loss: 1.38879 | Time: 0m 0s\n",
            "Epoch: 99 | Train acc:  100.00 | loss: 0.00012 | Val acc:  50.00 | loss: 1.40917 | Time: 0m 0s\n",
            "Epoch: 100 | Train acc:  100.00 | loss: 0.00051 | Val acc:  50.00 | loss: 1.32127 | Time: 0m 0s\n",
            "Epoch: 101 | Train acc:  100.00 | loss: 0.00018 | Val acc:  50.00 | loss: 1.31463 | Time: 0m 0s\n",
            "Epoch: 102 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.30193 | Time: 0m 0s\n",
            "Epoch: 103 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.24968 | Time: 0m 0s\n",
            "Epoch: 104 | Train acc:  100.00 | loss: 0.00013 | Val acc:  50.00 | loss: 1.21693 | Time: 0m 0s\n",
            "Epoch: 105 | Train acc:  100.00 | loss: 0.00019 | Val acc:  50.00 | loss: 1.32087 | Time: 0m 0s\n",
            "Epoch: 106 | Train acc:  100.00 | loss: 0.00033 | Val acc:  50.00 | loss: 1.41153 | Time: 0m 0s\n",
            "Epoch: 107 | Train acc:  100.00 | loss: 0.00028 | Val acc:  50.00 | loss: 1.30548 | Time: 0m 0s\n",
            "Epoch: 108 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.20250 | Time: 0m 0s\n",
            "Epoch: 109 | Train acc:  100.00 | loss: 0.00037 | Val acc:  50.00 | loss: 1.39069 | Time: 0m 0s\n",
            "Epoch: 110 | Train acc:  100.00 | loss: 0.00024 | Val acc:  50.00 | loss: 1.30846 | Time: 0m 0s\n",
            "Epoch: 111 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.30017 | Time: 0m 0s\n",
            "Epoch: 112 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.42925 | Time: 0m 0s\n",
            "Epoch: 113 | Train acc:  100.00 | loss: 0.00035 | Val acc:  50.00 | loss: 1.29045 | Time: 0m 0s\n",
            "Epoch: 114 | Train acc:  100.00 | loss: 0.00039 | Val acc:  50.00 | loss: 1.36309 | Time: 0m 0s\n",
            "Epoch: 115 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.36271 | Time: 0m 0s\n",
            "Epoch: 116 | Train acc:  100.00 | loss: 0.00019 | Val acc:  50.00 | loss: 1.28851 | Time: 0m 0s\n",
            "Epoch: 117 | Train acc:  100.00 | loss: 0.00023 | Val acc:  50.00 | loss: 1.35728 | Time: 0m 0s\n",
            "Epoch: 118 | Train acc:  100.00 | loss: 0.00041 | Val acc:  50.00 | loss: 1.33178 | Time: 0m 0s\n",
            "Epoch: 119 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.23762 | Time: 0m 0s\n",
            "Epoch: 120 | Train acc:  100.00 | loss: 0.00025 | Val acc:  50.00 | loss: 1.31781 | Time: 0m 0s\n",
            "Epoch: 121 | Train acc:  100.00 | loss: 0.00023 | Val acc:  50.00 | loss: 1.27146 | Time: 0m 0s\n",
            "Epoch: 122 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.45156 | Time: 0m 0s\n",
            "Epoch: 123 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.35690 | Time: 0m 0s\n",
            "Epoch: 124 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.31654 | Time: 0m 0s\n",
            "Epoch: 125 | Train acc:  100.00 | loss: 0.00028 | Val acc:  50.00 | loss: 1.36212 | Time: 0m 0s\n",
            "Epoch: 126 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.34129 | Time: 0m 0s\n",
            "Epoch: 127 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.31337 | Time: 0m 0s\n",
            "Epoch: 128 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.38008 | Time: 0m 0s\n",
            "Epoch: 129 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.32489 | Time: 0m 0s\n",
            "Epoch: 130 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.20015 | Time: 0m 0s\n",
            "Epoch: 131 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.30245 | Time: 0m 0s\n",
            "Epoch: 132 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.32927 | Time: 0m 0s\n",
            "Epoch: 133 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.37874 | Time: 0m 0s\n",
            "Epoch: 134 | Train acc:  100.00 | loss: 0.00025 | Val acc:  50.00 | loss: 1.31982 | Time: 0m 0s\n",
            "Epoch: 135 | Train acc:  100.00 | loss: 0.00018 | Val acc:  50.00 | loss: 1.54066 | Time: 0m 0s\n",
            "Epoch: 136 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.39185 | Time: 0m 0s\n",
            "Epoch: 137 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.35070 | Time: 0m 0s\n",
            "Epoch: 138 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.44778 | Time: 0m 0s\n",
            "Epoch: 139 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 1.22645 | Time: 0m 0s\n",
            "Epoch: 140 | Train acc:  100.00 | loss: 0.00011 | Val acc:  50.00 | loss: 1.35125 | Time: 0m 0s\n",
            "Epoch: 141 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.40009 | Time: 0m 0s\n",
            "Epoch: 142 | Train acc:  100.00 | loss: 0.00021 | Val acc:  50.00 | loss: 1.34434 | Time: 0m 0s\n",
            "Epoch: 143 | Train acc:  100.00 | loss: 0.00017 | Val acc:  50.00 | loss: 1.35368 | Time: 0m 0s\n",
            "Epoch: 144 | Train acc:  100.00 | loss: 0.00013 | Val acc:  50.00 | loss: 1.40459 | Time: 0m 0s\n",
            "Epoch: 145 | Train acc:  100.00 | loss: 0.00026 | Val acc:  50.00 | loss: 1.29836 | Time: 0m 0s\n",
            "Epoch: 146 | Train acc:  100.00 | loss: 0.00019 | Val acc:  50.00 | loss: 1.48031 | Time: 0m 0s\n",
            "Epoch: 147 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.42175 | Time: 0m 0s\n",
            "Epoch: 148 | Train acc:  100.00 | loss: 0.00024 | Val acc:  50.00 | loss: 1.36042 | Time: 0m 0s\n",
            "Epoch: 149 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.37899 | Time: 0m 0s\n",
            "Epoch: 150 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.35925 | Time: 0m 0s\n",
            "Epoch: 151 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.28740 | Time: 0m 0s\n",
            "Epoch: 152 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.41839 | Time: 0m 0s\n",
            "Epoch: 153 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.41245 | Time: 0m 0s\n",
            "Epoch: 154 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.21909 | Time: 0m 0s\n",
            "Epoch: 155 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.44376 | Time: 0m 0s\n",
            "Epoch: 156 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.39673 | Time: 0m 0s\n",
            "Epoch: 157 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.48656 | Time: 0m 0s\n",
            "Epoch: 158 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.45768 | Time: 0m 0s\n",
            "Epoch: 159 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.32006 | Time: 0m 0s\n",
            "Epoch: 160 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.31591 | Time: 0m 0s\n",
            "Epoch: 161 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.34844 | Time: 0m 0s\n",
            "Epoch: 162 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.29071 | Time: 0m 0s\n",
            "Epoch: 163 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.34380 | Time: 0m 0s\n",
            "Epoch: 164 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.44195 | Time: 0m 0s\n",
            "Epoch: 165 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.47553 | Time: 0m 0s\n",
            "Epoch: 166 | Train acc:  100.00 | loss: 0.00014 | Val acc:  50.00 | loss: 1.42231 | Time: 0m 0s\n",
            "Epoch: 167 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.37635 | Time: 0m 0s\n",
            "Epoch: 168 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.32857 | Time: 0m 0s\n",
            "Epoch: 169 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.37353 | Time: 0m 0s\n",
            "Epoch: 170 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.42196 | Time: 0m 0s\n",
            "Epoch: 171 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.32753 | Time: 0m 0s\n",
            "Epoch: 172 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 1.39979 | Time: 0m 0s\n",
            "Epoch: 173 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.42367 | Time: 0m 0s\n",
            "Epoch: 174 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.39103 | Time: 0m 0s\n",
            "Epoch: 175 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.26094 | Time: 0m 0s\n",
            "Epoch: 176 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.40983 | Time: 0m 0s\n",
            "Epoch: 177 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.35683 | Time: 0m 0s\n",
            "Epoch: 178 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.45037 | Time: 0m 0s\n",
            "Epoch: 179 | Train acc:  100.00 | loss: 0.00012 | Val acc:  50.00 | loss: 1.26973 | Time: 0m 0s\n",
            "Epoch: 180 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 1.30192 | Time: 0m 0s\n",
            "Epoch: 181 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.40584 | Time: 0m 0s\n",
            "Epoch: 182 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.39920 | Time: 0m 0s\n",
            "Epoch: 183 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.34876 | Time: 0m 0s\n",
            "Epoch: 184 | Train acc:  100.00 | loss: 0.00007 | Val acc:  50.00 | loss: 1.39519 | Time: 0m 0s\n",
            "Epoch: 185 | Train acc:  100.00 | loss: 0.00007 | Val acc:  50.00 | loss: 1.44183 | Time: 0m 0s\n",
            "Epoch: 186 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.33585 | Time: 0m 0s\n",
            "Epoch: 187 | Train acc:  100.00 | loss: 0.00018 | Val acc:  50.00 | loss: 1.33763 | Time: 0m 0s\n",
            "Epoch: 188 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 1.33915 | Time: 0m 0s\n",
            "Epoch: 189 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 1.38788 | Time: 0m 0s\n",
            "Epoch: 190 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.44981 | Time: 0m 0s\n",
            "Epoch: 191 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.41857 | Time: 0m 0s\n",
            "Epoch: 192 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.43864 | Time: 0m 0s\n",
            "Epoch: 193 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.42927 | Time: 0m 0s\n",
            "Epoch: 194 | Train acc:  100.00 | loss: 0.00000 | Val acc:  50.00 | loss: 1.37666 | Time: 0m 0s\n",
            "Epoch: 195 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.41868 | Time: 0m 0s\n",
            "Epoch: 196 | Train acc:  100.00 | loss: 0.00001 | Val acc:  50.00 | loss: 1.31640 | Time: 0m 0s\n",
            "Epoch: 197 | Train acc:  100.00 | loss: 0.00009 | Val acc:  50.00 | loss: 1.41134 | Time: 0m 0s\n",
            "Epoch: 198 | Train acc:  100.00 | loss: 0.00013 | Val acc:  50.00 | loss: 1.52187 | Time: 0m 0s\n",
            "Epoch: 199 | Train acc:  100.00 | loss: 0.00013 | Val acc:  50.00 | loss: 1.45054 | Time: 0m 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaCbs-aiP-kb",
        "outputId": "c8f8c2bc-ed09-4573-f56d-9725a37086fd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(X_test_tensor.float().to(device))\n",
        "#testloss = criterion(yhat, y_test_tensor.type(torch.LongTensor).to(device))\n",
        "_, predicted = torch.max(yhat.data, 1)\n",
        "\n",
        "predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEAbteS8D1zn",
        "outputId": "6622a18d-e6a2-46c1-c186-414d9d7c67db"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax  = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(valid_losses, label = 'valid loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "y0NyPhdeK52s",
        "outputId": "e2aca953-bc8b-44c3-8ddf-eebac7ade5b5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G2zb7hjM5F6",
        "outputId": "ea38f628-7ff7-4c91-d888-8a1500f16862"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7330622673034668,\n",
              " 4.9976935386657715,\n",
              " 0.647854745388031,\n",
              " 3.6984028816223145,\n",
              " 0.07413646578788757,\n",
              " 0.6568796038627625,\n",
              " 1.1440986394882202,\n",
              " 0.0031090022530406713,\n",
              " 0.009606783278286457,\n",
              " 0.0781388208270073,\n",
              " 0.21781522035598755,\n",
              " 0.004768801387399435,\n",
              " 0.19473016262054443,\n",
              " 0.1067170798778534,\n",
              " 0.007637417409569025,\n",
              " 0.002221743343397975,\n",
              " 0.005793724674731493,\n",
              " 0.007281288504600525,\n",
              " 0.0350443534553051,\n",
              " 0.02361218072474003,\n",
              " 7.322988676605746e-05,\n",
              " 0.00044936430640518665,\n",
              " 0.010352441109716892,\n",
              " 0.011588053777813911,\n",
              " 0.004076866898685694,\n",
              " 0.009598702192306519,\n",
              " 0.0005236411234363914,\n",
              " 0.002169723156839609,\n",
              " 0.0020262831822037697,\n",
              " 2.419916563667357e-05,\n",
              " 0.016931824386119843,\n",
              " 0.0001514920877525583,\n",
              " 0.00028159026987850666,\n",
              " 0.013361495919525623,\n",
              " 0.0005504529108293355,\n",
              " 0.00014350962010212243,\n",
              " 0.0002670424000825733,\n",
              " 0.008375713601708412,\n",
              " 7.537392230005935e-05,\n",
              " 0.0010795374400913715,\n",
              " 0.0007641829433850944,\n",
              " 5.32465310243424e-06,\n",
              " 0.007145722862333059,\n",
              " 0.0009872609516605735,\n",
              " 0.0011899707606062293,\n",
              " 0.0003960653266403824,\n",
              " 0.0043620881624519825,\n",
              " 5.185589088796405e-06,\n",
              " 0.000451483327196911,\n",
              " 0.0036375951021909714,\n",
              " 0.0013482015347108245,\n",
              " 2.5510227715130895e-05,\n",
              " 0.0010631391778588295,\n",
              " 0.00045118809794075787,\n",
              " 0.0020128509495407343,\n",
              " 0.00026505126152187586,\n",
              " 0.0005045980797149241,\n",
              " 0.00018203006766270846,\n",
              " 0.00013329599460121244,\n",
              " 0.005063851363956928,\n",
              " 0.00015836213424336165,\n",
              " 0.0004787996003869921,\n",
              " 0.002968729706481099,\n",
              " 0.00017121806740760803,\n",
              " 0.0012987939408048987,\n",
              " 0.0016830364475026727,\n",
              " 0.0007570732850581408,\n",
              " 2.241100446553901e-05,\n",
              " 0.0001315072731813416,\n",
              " 0.00046895831474103034,\n",
              " 0.0013699164846912026,\n",
              " 0.0016758458223193884,\n",
              " 0.0012749747838824987,\n",
              " 0.0016934730811044574,\n",
              " 2.419897100480739e-05,\n",
              " 0.00022376592096406966,\n",
              " 0.0025358623825013638,\n",
              " 0.00012117446021875367,\n",
              " 0.0006580543704330921,\n",
              " 2.2112955775810406e-05,\n",
              " 0.0012462370796129107,\n",
              " 0.0006702432874590158,\n",
              " 0.0012260460061952472,\n",
              " 1.209960100823082e-05,\n",
              " 0.0004522159870248288,\n",
              " 0.001056196866557002,\n",
              " 0.00011521828128024936,\n",
              " 0.0019160814117640257,\n",
              " 0.0012905494077131152,\n",
              " 0.0009399994160048664,\n",
              " 0.0002111677749780938,\n",
              " 0.0009145500953309238,\n",
              " 0.0012572709238156676,\n",
              " 1.9629502276075073e-05,\n",
              " 0.0015231553697958589,\n",
              " 1.7285052308579907e-05,\n",
              " 0.00012352355406619608,\n",
              " 0.0026249096263200045,\n",
              " 0.00010246485908282921,\n",
              " 2.521226997487247e-05,\n",
              " 0.00011538231774466112,\n",
              " 0.0011992433574050665,\n",
              " 0.0005531222559511662,\n",
              " 0.0011781869689002633,\n",
              " 0.0018223231891170144,\n",
              " 0.00047322563477791846,\n",
              " 8.331961726071313e-05,\n",
              " 0.0001727072667563334,\n",
              " 0.00122262560762465,\n",
              " 0.00022010468819644302,\n",
              " 0.0010873513529077172,\n",
              " 2.2053329303162172e-05,\n",
              " 0.00022260886908043176,\n",
              " 1.2516781680460554e-05,\n",
              " 0.0014613681705668569,\n",
              " 0.0010203022975474596,\n",
              " 0.001107936492189765,\n",
              " 0.00014468919835053384,\n",
              " 0.0006787709426134825,\n",
              " 0.000502526352647692,\n",
              " 0.0002854264748748392,\n",
              " 0.00108666205778718,\n",
              " 9.249743743566796e-05,\n",
              " 0.0010136028286069632,\n",
              " 0.0003413116792216897,\n",
              " 8.721373887965456e-05,\n",
              " 0.0009567972156219184,\n",
              " 0.0009000247227959335,\n",
              " 0.0012025105534121394,\n",
              " 0.00026022171368822455,\n",
              " 0.00013297692930791527,\n",
              " 0.000697368464898318,\n",
              " 0.00013031369599048048,\n",
              " 1.9351349692442454e-05,\n",
              " 0.0016040318878367543,\n",
              " 0.00044618011452257633,\n",
              " 0.00040141059434972703,\n",
              " 2.3404376406688243e-05,\n",
              " 0.0009379339753650129,\n",
              " 0.0009441965958103538,\n",
              " 9.58732925937511e-05,\n",
              " 0.0007180881802923977,\n",
              " 0.0001419495529262349,\n",
              " 0.0017647661734372377,\n",
              " 0.0008653795230202377,\n",
              " 0.0001234808296430856,\n",
              " 1.7046691937139258e-05,\n",
              " 0.001121704000979662,\n",
              " 0.0010421245824545622,\n",
              " 0.0006270291632972658,\n",
              " 0.0002692269627004862,\n",
              " 3.337803718750365e-05,\n",
              " 0.0009829933987930417,\n",
              " 7.457999890903011e-05,\n",
              " 0.0006678912322968245,\n",
              " 2.0801677237614058e-05,\n",
              " 7.251333590829745e-05,\n",
              " 0.0001038978443830274,\n",
              " 0.0014577311230823398,\n",
              " 8.821411938697565e-06,\n",
              " 0.0010663463035598397,\n",
              " 0.0004173745692241937,\n",
              " 7.15993155608885e-05,\n",
              " 0.0001905800454551354,\n",
              " 0.0008075767545960844,\n",
              " 0.0003115604631602764,\n",
              " 0.0004526906704995781,\n",
              " 0.00010728002234827727,\n",
              " 0.0007118026842363179,\n",
              " 0.00010528832353884354,\n",
              " 0.0006714875344187021,\n",
              " 0.00010698021651478484,\n",
              " 0.00011875509517267346,\n",
              " 0.00048745027743279934,\n",
              " 0.000356670847395435,\n",
              " 0.0009770499309524894,\n",
              " 0.00027849210891872644,\n",
              " 6.198839855642291e-06,\n",
              " 0.00012730307935271412,\n",
              " 0.0017626227345317602,\n",
              " 0.0008380197104997933,\n",
              " 9.496903658146039e-06,\n",
              " 0.0007201738771982491,\n",
              " 1.5914258256088942e-05,\n",
              " 0.0008195709087885916,\n",
              " 0.0007482118089683354,\n",
              " 9.611146379029378e-05,\n",
              " 0.00010078131890622899,\n",
              " 0.0002934863732662052,\n",
              " 0.001113893580622971,\n",
              " 1.2437359146133531e-05,\n",
              " 0.00014172283408697695,\n",
              " 7.24329220247455e-05,\n",
              " 0.0006008998607285321,\n",
              " 0.0003133453137706965,\n",
              " 0.0006028382922522724,\n",
              " 7.088481652317569e-05,\n",
              " 8.423102553933859e-05,\n",
              " 0.0009440199355594814,\n",
              " 0.0003281431272625923,\n",
              " 0.0003853934758808464,\n",
              " 0.00030759492074139416,\n",
              " 0.0006774924113415182,\n",
              " 7.152528269216418e-06,\n",
              " 0.0007541528320871294,\n",
              " 0.00044339479063637555,\n",
              " 0.00022841396275907755,\n",
              " 1.5497184904234018e-06,\n",
              " 0.0007453390280716121,\n",
              " 0.00032111903419718146,\n",
              " 0.0002619552833493799,\n",
              " 1.645066549826879e-05,\n",
              " 0.0005547080072574317,\n",
              " 5.4514141083927825e-05,\n",
              " 2.229184792668093e-05,\n",
              " 0.0009239156497642398,\n",
              " 0.0006681294180452824,\n",
              " 0.00027104283799417317,\n",
              " 0.00039222126360982656,\n",
              " 1.490098838985432e-05,\n",
              " 1.2238638191774953e-05,\n",
              " 0.00046096148435026407,\n",
              " 0.0006003431044518948,\n",
              " 0.00015173466817941517,\n",
              " 0.00035589339677244425,\n",
              " 5.1812468882417306e-05,\n",
              " 0.0005602173041552305,\n",
              " 0.00028221600223332644,\n",
              " 0.00030373549088835716,\n",
              " 0.0002020433748839423,\n",
              " 0.0005875451606698334,\n",
              " 7.795773854013532e-05,\n",
              " 2.7020716970582725e-06,\n",
              " 8.713351417100057e-05,\n",
              " 0.0009321703691966832,\n",
              " 0.0003502728359308094,\n",
              " 0.00030135823180899024,\n",
              " 0.0004613798810169101,\n",
              " 0.00028430172824300826,\n",
              " 8.523415999661665e-06,\n",
              " 0.000532784906681627,\n",
              " 0.00022478665050584823,\n",
              " 1.8795068172039464e-05,\n",
              " 0.0004478523915167898,\n",
              " 0.0007061385549604893,\n",
              " 0.00017658369324635714,\n",
              " 7.179645035648718e-05,\n",
              " 7.080695650074631e-05,\n",
              " 0.0006132672424428165,\n",
              " 3.139170758004184e-06,\n",
              " 0.0001877880422398448,\n",
              " 0.0003562577476259321,\n",
              " 0.0005876240320503712,\n",
              " 0.0002458626695442945,\n",
              " 0.00018297640781383961,\n",
              " 1.7702328477753326e-05,\n",
              " 0.0001591107138665393,\n",
              " 0.0003212793671991676,\n",
              " 1.235789659403963e-05,\n",
              " 0.0006224202807061374,\n",
              " 0.00014962123532313854,\n",
              " 0.00022757385158911347,\n",
              " 7.572979666292667e-05,\n",
              " 0.0007769176736474037,\n",
              " 0.0005949933547526598,\n",
              " 0.00045110343489795923,\n",
              " 4.172311037109466e-06,\n",
              " 1.5914229152258486e-05,\n",
              " 3.8939713704166934e-05,\n",
              " 7.239515980472788e-05,\n",
              " 0.000564892718102783,\n",
              " 0.0003063336480408907,\n",
              " 0.00021828182798344642,\n",
              " 1.4503639249596745e-05,\n",
              " 0.00041326406062580645,\n",
              " 0.0003490312956273556,\n",
              " 0.00038078264333307743,\n",
              " 0.00024062639568001032,\n",
              " 0.00023793982109054923,\n",
              " 1.2695629266090691e-05,\n",
              " 8.265101314464118e-06,\n",
              " 0.00019268819596618414,\n",
              " 0.0006014458485879004,\n",
              " 8.439327211817726e-05,\n",
              " 0.00019745912868529558,\n",
              " 0.00042290135752409697,\n",
              " 4.649142738344381e-06,\n",
              " 0.0003320082032587379,\n",
              " 7.485615060431883e-05,\n",
              " 0.0007493851589970291,\n",
              " 5.38394961040467e-05,\n",
              " 1.692748446657788e-05,\n",
              " 0.00018458337581250817,\n",
              " 0.0002096245443681255,\n",
              " 7.553252362413332e-05,\n",
              " 0.0005655070417560637,\n",
              " 1.597381015017163e-05,\n",
              " 0.000647334789391607,\n",
              " 0.0001601030380697921,\n",
              " 0.00011401102528907359,\n",
              " 1.8199034457211383e-05,\n",
              " 0.0004888540715910494,\n",
              " 0.0002071632188744843,\n",
              " 0.00022406387142837048,\n",
              " 0.0006080984021537006,\n",
              " 6.476557609857991e-05,\n",
              " 1.2318182598392013e-05,\n",
              " 0.00018140365136787295,\n",
              " 0.00039332054439000785,\n",
              " 4.5615146518684924e-05,\n",
              " 0.00014239006850402802,\n",
              " 0.0003276972856838256,\n",
              " 4.490198989515193e-06,\n",
              " 0.00034971124841831625,\n",
              " 0.0003106640069745481,\n",
              " 5.221110404818319e-05,\n",
              " 0.0001825970393838361,\n",
              " 0.0003125868388451636,\n",
              " 0.0003388667246326804,\n",
              " 4.649141374102328e-06,\n",
              " 0.0005150700453668833,\n",
              " 1.24373946164269e-05,\n",
              " 7.863376958994195e-05,\n",
              " 0.000318195583531633,\n",
              " 0.00041790949762798846,\n",
              " 4.569672000798164e-06,\n",
              " 0.0001809941022656858,\n",
              " 0.00012110889656469226,\n",
              " 0.00021188876416999847,\n",
              " 6.281874811975285e-05,\n",
              " 0.0004154493799433112,\n",
              " 7.122393435565755e-05,\n",
              " 0.0003582440549507737,\n",
              " 0.0001427438110113144,\n",
              " 3.107256270595826e-05,\n",
              " 0.0002493293141014874,\n",
              " 1.0688997463148553e-05,\n",
              " 0.0004336087731644511,\n",
              " 0.00013175077037885785,\n",
              " 8.737300231587142e-05,\n",
              " 0.00017392344307154417,\n",
              " 0.00010254522931063548,\n",
              " 0.00033859023824334145,\n",
              " 0.00011967832688242197,\n",
              " 0.00012511713430285454,\n",
              " 0.00028633777401410043,\n",
              " 0.00021274620667099953,\n",
              " 1.8715636542765424e-05,\n",
              " 0.0001644800795475021,\n",
              " 0.00033409634488634765,\n",
              " 1.0172431757382583e-05,\n",
              " 0.00014410365838557482,\n",
              " 5.793220771010965e-05,\n",
              " 0.00013091454457025975,\n",
              " 0.0004124495608266443,\n",
              " 9.238635357178282e-06,\n",
              " 0.00019174523185938597,\n",
              " 0.00014409614959731698,\n",
              " 0.00026049246662296355,\n",
              " 1.275523300137138e-05,\n",
              " 0.00034564314410090446,\n",
              " 0.00014266454672906548,\n",
              " 5.5110034736571833e-05,\n",
              " 6.174790905788541e-05,\n",
              " 0.0002928810426965356,\n",
              " 2.861015673261136e-06,\n",
              " 5.503172360477038e-05,\n",
              " 0.0004000917251687497,\n",
              " 0.00022939650807529688,\n",
              " 0.00038204583688639104,\n",
              " 9.536657671560533e-06,\n",
              " 2.9802276912960224e-06,\n",
              " 6.512178515549749e-05,\n",
              " 0.0003150707925669849,\n",
              " 9.54741844907403e-05,\n",
              " 0.00023681689344812185,\n",
              " 3.627749902079813e-05,\n",
              " 0.00030932363006286323,\n",
              " 0.0002672943810466677,\n",
              " 1.2516864444478415e-05,\n",
              " 2.9801312848576345e-05,\n",
              " 0.00037786419852636755,\n",
              " 4.203930438961834e-05,\n",
              " 0.00013319904974196106,\n",
              " 3.592046778067015e-05,\n",
              " 1.0967173693643417e-05,\n",
              " 0.00034199332003481686,\n",
              " 0.00039622181793674827,\n",
              " 0.0001653480576351285,\n",
              " 0.00011299165635136887,\n",
              " 0.0002647811488714069,\n",
              " 9.750800381880254e-05,\n",
              " 0.00016773852985352278,\n",
              " 7.112759249139344e-06,\n",
              " 4.549556251731701e-05,\n",
              " 0.0004982048412784934,\n",
              " 0.00014207202184479684,\n",
              " 0.00026755835278891027,\n",
              " 6.587977259187028e-05,\n",
              " 0.00011925566650461406,\n",
              " 0.0001839954493334517,\n",
              " 9.020107427204493e-06,\n",
              " 2.7337648134562187e-05,\n",
              " 0.000506863696500659,\n",
              " 0.00024611703702248633,\n",
              " 0.00013861498155165464,\n",
              " 2.8490316253737547e-05,\n",
              " 0.00018362242553848773,\n",
              " 0.0002820821537170559,\n",
              " 0.00023751390108373016,\n",
              " 9.059830517799128e-06,\n",
              " 1.0788347935886122e-05,\n",
              " 6.11506329732947e-05,\n",
              " 0.00011681105388561264,\n",
              " 0.0002475476067047566,\n",
              " 0.00010691930947359651,\n",
              " 0.00014509703032672405,\n",
              " 8.98033886187477e-06,\n",
              " 0.0002862721448764205,\n",
              " 0.00013332004891708493,\n",
              " 0.0002305281232111156,\n",
              " 9.89427644526586e-06,\n",
              " 0.00013000312901567668,\n",
              " 0.00018968613585457206,\n",
              " 0.0001029024351737462,\n",
              " 0.00016539810167159885,\n",
              " 8.821395567792933e-06,\n",
              " 0.0003285152488388121,\n",
              " 9.02707688510418e-05,\n",
              " 0.00014708307571709156,\n",
              " 3.294051930424757e-05,\n",
              " 0.0002847099385689944,\n",
              " 0.00023075402714312077,\n",
              " 9.19379890547134e-05,\n",
              " 0.00013782574387732893,\n",
              " 4.6490138629451394e-05,\n",
              " 0.00010143347753910348,\n",
              " 0.0001271331711905077,\n",
              " 3.1072959245648235e-05,\n",
              " 0.0003707567520905286,\n",
              " 9.218772902386263e-06,\n",
              " 0.00024087447673082352,\n",
              " 2.6304835046175867e-05,\n",
              " 0.00023510513710789382,\n",
              " 0.0001037032634485513,\n",
              " 0.00012097814033040777,\n",
              " 0.00020736413716804236,\n",
              " 4.249642006470822e-05,\n",
              " 0.00010353855759603903,\n",
              " 4.259544948581606e-05,\n",
              " 0.0001861263153841719,\n",
              " 0.00014553373330272734,\n",
              " 0.00010580200614640489,\n",
              " 4.7443434596061707e-05,\n",
              " 2.936410419351887e-05,\n",
              " 0.00035361311165615916,\n",
              " 7.390910923277261e-06,\n",
              " 4.017188257421367e-05,\n",
              " 0.00013603828847408295,\n",
              " 0.00038596661761403084,\n",
              " 6.933429540367797e-05,\n",
              " 3.238415592932142e-05,\n",
              " 0.00026689140941016376,\n",
              " 5.3522402595262975e-05,\n",
              " 0.00018452816584613174,\n",
              " 9.20225284062326e-05,\n",
              " 8.980371603684034e-06,\n",
              " 0.00019327065092511475,\n",
              " 0.00010890566773014143,\n",
              " 8.702223567524925e-06,\n",
              " 0.0001004054574877955,\n",
              " 0.00022507987159769982,\n",
              " 7.549857400590554e-06,\n",
              " 4.1681836592033505e-05,\n",
              " 0.00012519342999439687,\n",
              " 0.00040873157558962703,\n",
              " 4.394644565763883e-05,\n",
              " 0.00014969588664826006,\n",
              " 0.00017600860155653208,\n",
              " 3.4808123018592596e-05,\n",
              " 9.417451110493857e-06,\n",
              " 0.000122969169751741,\n",
              " 9.563861385686323e-05,\n",
              " 0.00024914584355428815,\n",
              " 0.00013464865332935005,\n",
              " 0.0001021126881823875,\n",
              " 8.026700925256591e-06,\n",
              " 0.0002343145606573671,\n",
              " 6.7551395659393165e-06,\n",
              " 8.650030213175341e-05,\n",
              " 0.00021003528672736138,\n",
              " 0.00014714349526911974,\n",
              " 0.0001529679138911888,\n",
              " 1.434471778338775e-05,\n",
              " 0.0001722610177239403,\n",
              " 5.483340282808058e-05,\n",
              " 0.00024671442224644125,\n",
              " 3.818532422883436e-05,\n",
              " 7.732149242656305e-05,\n",
              " 1.1265176908636931e-05,\n",
              " 8.669830276630819e-05,\n",
              " 8.534369408152997e-05,\n",
              " 2.9046508643659763e-05,\n",
              " 0.0002764939854387194,\n",
              " 0.0002069894107989967,\n",
              " 8.18563967186492e-06,\n",
              " 0.00011756780440919101,\n",
              " 4.005326991318725e-05,\n",
              " 5.499330654856749e-05,\n",
              " 0.0002661402686499059,\n",
              " 7.271728009072831e-06,\n",
              " 8.93393880687654e-05,\n",
              " 0.000251887395279482,\n",
              " 7.557138451375067e-05,\n",
              " 2.964220220746938e-05,\n",
              " 1.1563222869881429e-05,\n",
              " 0.00022112812439445406,\n",
              " 0.00012332132610026747,\n",
              " 4.323175016907044e-05,\n",
              " 1.1980449926340953e-05,\n",
              " 2.8887550797662698e-05,\n",
              " 0.00024803285486996174,\n",
              " 7.862988422857597e-05,\n",
              " 1.1324778824928217e-05,\n",
              " 0.00018402142450213432,\n",
              " 6.170610868139192e-05,\n",
              " 0.00010389972158009186,\n",
              " 7.510129762522411e-06,\n",
              " 1.8278733477927744e-06,\n",
              " 0.00017080425459425896,\n",
              " 0.00010080223000841215,\n",
              " 5.161560693522915e-05,\n",
              " 2.356306322326418e-05,\n",
              " 6.353267963277176e-05,\n",
              " 0.00018514652037993073,\n",
              " 3.337793896207586e-05,\n",
              " 7.31498803361319e-05,\n",
              " 8.713315037311986e-05,\n",
              " 3.0994324333732948e-06,\n",
              " 0.0002476111985743046,\n",
              " 3.0556126148439944e-05,\n",
              " 7.708263001404703e-05,\n",
              " 0.00010079931234940886,\n",
              " 0.00017836490587797016,\n",
              " 2.8092690627090633e-05,\n",
              " 8.185636943380814e-06,\n",
              " 0.00021108968940097839,\n",
              " 0.00010417938756290823,\n",
              " 0.00015633838484063745,\n",
              " 9.031364606926218e-05,\n",
              " 2.9404880024230806e-06,\n",
              " 0.00010960257350234315,\n",
              " 2.145716098311823e-05,\n",
              " 8.665616769576445e-05,\n",
              " 0.0001870496926130727,\n",
              " 4.428429383551702e-05,\n",
              " 3.1311294151237234e-05,\n",
              " 0.00021100490994285792,\n",
              " 2.3205540855997242e-05,\n",
              " 7.718222332186997e-05,\n",
              " 2.288775249326136e-05,\n",
              " 0.0001430227275704965,\n",
              " 7.207445742096752e-05,\n",
              " 0.00011210920638404787,\n",
              " 0.00017261355242226273,\n",
              " 2.4913846573326737e-05,\n",
              " 6.500345625681803e-05,\n",
              " 9.94741203612648e-05,\n",
              " 9.416623652214184e-05,\n",
              " 6.218213093234226e-05,\n",
              " 2.940380545624066e-05,\n",
              " 0.00020857571507804096,\n",
              " 2.0980463887099177e-05,\n",
              " 0.00011232413817197084,\n",
              " 5.642116229864769e-05,\n",
              " 0.00016775894619058818,\n",
              " 0.00011247554357396439,\n",
              " 1.974863698706031e-05,\n",
              " 8.121610881062225e-05,\n",
              " 0.00013164972187951207,\n",
              " 9.659133502282202e-05,\n",
              " 2.7020712423109217e-06,\n",
              " 2.4159366148523986e-05,\n",
              " 0.00026204041205346584,\n",
              " 0.00010974353790516034,\n",
              " 4.887422619503923e-05,\n",
              " 2.6225982310279505e-06,\n",
              " 0.00018771992472466081,\n",
              " 7.326705235755071e-05,\n",
              " 8.153373346431181e-05,\n",
              " 0.00010675926023395732,\n",
              " 9.477085768594407e-06,\n",
              " 2.276847772009205e-05,\n",
              " 2.1616213416564278e-05,\n",
              " 7.04061021679081e-05,\n",
              " 0.00023659647558815777,\n",
              " 0.0001338987349299714,\n",
              " 0.00013110907457303256,\n",
              " 2.3841807887947652e-06,\n",
              " 4.017238097731024e-05,\n",
              " 5.332217551767826e-05,\n",
              " 5.940135088167153e-05,\n",
              " 0.00015864067245274782,\n",
              " 2.1755333364126272e-05,\n",
              " 8.50293436087668e-05,\n",
              " 2.0901012248941697e-05,\n",
              " 4.0608374547446147e-05,\n",
              " 0.00014380764332599938,\n",
              " 9.417465662409086e-06,\n",
              " 0.00010719184501795098,\n",
              " 0.0001239690463989973,\n",
              " 2.694089744181838e-05,\n",
              " 0.00010596047650324181,\n",
              " 8.276422158814967e-05,\n",
              " 7.430671303154668e-06,\n",
              " 9.685241093393415e-05,\n",
              " 0.0001455359742976725,\n",
              " 6.492397369584069e-05,\n",
              " 2.3880857042968273e-05,\n",
              " 1.8477128833183087e-05,\n",
              " 4.0648632420925424e-05,\n",
              " 0.0001853846333688125,\n",
              " 3.86623723898083e-05,\n",
              " 3.1590407161274925e-06,\n",
              " 6.369246693793684e-05,\n",
              " 5.722016339859692e-06,\n",
              " 0.00013492221478372812,\n",
              " 9.470661461818963e-05,\n",
              " 6.778637907700613e-05,\n",
              " 0.0001004788136924617,\n",
              " 4.9866255722008646e-05,\n",
              " 2.7596464860835113e-05,\n",
              " 1.6013555068639107e-05,\n",
              " 3.576104427338578e-05,\n",
              " 6.524175114464015e-05,\n",
              " 0.000154657696839422,\n",
              " 2.634455267980229e-05,\n",
              " 7.828027264622506e-06,\n",
              " 0.0001918660127557814,\n",
              " 1.7523459973745048e-05,\n",
              " 3.222550731152296e-05,\n",
              " 4.12051122111734e-05,\n",
              " 9.221587242791429e-05,\n",
              " 9.19019294087775e-05,\n",
              " 7.073050710459938e-06,\n",
              " 0.00011272113624727353,\n",
              " 6.254010804696009e-05,\n",
              " 5.209302980802022e-05,\n",
              " 9.896940173348412e-05,\n",
              " 8.073838398559019e-05,\n",
              " 4.44228935521096e-05,\n",
              " 2.5450786779401824e-05,\n",
              " 2.217233668488916e-05,\n",
              " 0.00010366075002821162,\n",
              " 4.967025688529247e-06,\n",
              " 0.00013970320287626237,\n",
              " 6.047678107279353e-05,\n",
              " 5.669991878676228e-05,\n",
              " 8.185657861758955e-06,\n",
              " 0.00014630801160819829,\n",
              " 7.195818034233525e-05,\n",
              " 0.00013783032773062587,\n",
              " 1.5338053344748914e-05,\n",
              " 2.0861582470388385e-06,\n",
              " 7.839523459551856e-05,\n",
              " 5.086238616058836e-06,\n",
              " 3.814595038420521e-05,\n",
              " 0.00013677592505700886,\n",
              " 0.00011172574158990756,\n",
              " 1.9907625755877234e-05,\n",
              " 6.774444773327559e-05,\n",
              " 5.155558028491214e-05,\n",
              " 2.4159124222933315e-05,\n",
              " 3.8979414966888726e-05,\n",
              " 0.00014466395077761263,\n",
              " 2.6344825528212823e-05,\n",
              " 0.0001016388923744671,\n",
              " 8.303890354000032e-05,\n",
              " 9.8148348115501e-06,\n",
              " 2.7298290660837665e-05,\n",
              " 6.14684249740094e-05,\n",
              " 0.00012229586718603969,\n",
              " 1.5894548823780497e-06,\n",
              " 3.766913141589612e-05,\n",
              " 7.100282527972013e-05,\n",
              " 2.4159015083569102e-05,\n",
              " 3.743000706890598e-05,\n",
              " 0.000151496147736907,\n",
              " 9.094557026401162e-05,\n",
              " 3.882064265781082e-05,\n",
              " 2.6146108211833052e-05,\n",
              " 8.415479533141479e-05,\n",
              " 7.79148904257454e-05,\n",
              " 9.929487714543939e-05,\n",
              " 1.6212177797569893e-05,\n",
              " 6.1988448578631505e-06,\n",
              " 0.00014895782805979252,\n",
              " 1.3947318620921578e-05,\n",
              " 3.897930946550332e-05,\n",
              " 7.867773092584684e-06,\n",
              " 0.00013207148003857583,\n",
              " 4.883379733655602e-05,\n",
              " 1.8516699128667824e-05,\n",
              " 9.059858712134883e-06,\n",
              " 7.362729957094416e-05,\n",
              " 6.861790461698547e-05,\n",
              " 2.0344743461464532e-05,\n",
              " 2.7477077310322784e-05,\n",
              " 4.704486855189316e-05,\n",
              " 3.087461300310679e-05,\n",
              " 8.681379404151812e-05,\n",
              " 5.227070869295858e-05,\n",
              " 0.00011470822937553748,\n",
              " 2.4437284082523547e-05,\n",
              " 1.3708850019611418e-05,\n",
              " 5.3820753237232566e-05,\n",
              " 2.515252708690241e-05,\n",
              " 6.901781307533383e-05,\n",
              " 3.2224808819592e-05,\n",
              " 0.00012122153566451743,\n",
              " 9.436666005058214e-05,\n",
              " 3.7271336623234674e-05,\n",
              " 1.390773491038999e-06,\n",
              " 8.463235280942172e-05,\n",
              " 0.00010390338866272941,\n",
              " 6.810419290559366e-05,\n",
              " 5.086238161311485e-06,\n",
              " 8.881024768925272e-06,\n",
              " 1.947081727848854e-06,\n",
              " 4.164240090176463e-05,\n",
              " 9.575230069458485e-05,\n",
              " 9.321608376922086e-05,\n",
              " 3.91779794881586e-05,\n",
              " 3.802599530899897e-05,\n",
              " 9.07868379727006e-05,\n",
              " 4.005339360446669e-05,\n",
              " 1.4066537005419377e-05,\n",
              " 8.709110989002511e-05,\n",
              " 5.77329374209512e-05,\n",
              " 6.943666085135192e-05,\n",
              " 5.404126568464562e-06,\n",
              " 1.8636032109498046e-05,\n",
              " 0.00011045613791793585,\n",
              " 6.67543281451799e-05,\n",
              " 1.9072969735134393e-05,\n",
              " 6.0634585679508746e-05,\n",
              " 0.00010048461990663782,\n",
              " 9.59629142016638e-06,\n",
              " 4.6051722165429965e-05,\n",
              " 1.8198983525508083e-05,\n",
              " 1.0172410839004442e-05,\n",
              " 0.0001835003204178065,\n",
              " 6.556476819241652e-06,\n",
              " 5.3959291108185425e-05,\n",
              " 2.9046359486528672e-05,\n",
              " 0.00015102166798897088,\n",
              " 3.218531855964102e-05,\n",
              " 8.702175364305731e-06,\n",
              " 5.757564213126898e-05,\n",
              " 9.613345173420385e-05,\n",
              " 9.972985571948811e-05,\n",
              " 4.6011893573449925e-05,\n",
              " 2.737792965490371e-05,\n",
              " 5.36438938070205e-06,\n",
              " 1.78413865796756e-05,\n",
              " 7.708097837166861e-05,\n",
              " 4.338952203397639e-05,\n",
              " 5.60265252715908e-05,\n",
              " 5.483609129441902e-06,\n",
              " 6.421090802177787e-05,\n",
              " 9.190304990625009e-05,\n",
              " 6.198853952810168e-06,\n",
              " 4.716412513516843e-05,\n",
              " 2.479508657415863e-05,\n",
              " 7.572985487058759e-05,\n",
              " 5.63240100746043e-05,\n",
              " 1.3748652236245107e-05,\n",
              " 0.00011566242756089196,\n",
              " 3.897948408848606e-05,\n",
              " 3.6954811548639555e-06,\n",
              " 4.823791095986962e-05,\n",
              " 7.918706251075491e-05,\n",
              " 3.051627572858706e-05,\n",
              " 1.29341233332525e-05,\n",
              " 3.103306517004967e-05,\n",
              " 7.064559031277895e-05,\n",
              " 4.335079211159609e-05,\n",
              " 9.238663551514037e-06,\n",
              " 1.9391025489312597e-05,\n",
              " 4.192033884464763e-05,\n",
              " 3.2225201721303165e-05,\n",
              " 8.999505371320993e-05,\n",
              " 7.589281449327245e-05,\n",
              " 8.026710020203609e-06,\n",
              " 2.980227463922347e-06,\n",
              " 0.00012837584654334933,\n",
              " 5.99198137933854e-05,\n",
              " 9.814812074182555e-06,\n",
              " 6.874367045384133e-06,\n",
              " 0.00012939015869051218]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOeze6bED12x",
        "outputId": "aba8c8c9-89e5-4ecc-8e29-39cc47a2fe85"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = eegConv1d().to(device)\n",
        "model.load_state_dict(torch.load('./models/CNN1D.pt'))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    total   = 0\n",
        "    correct = 0\n",
        "    for X_test, y_test in test_loader:\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        predictions   = model(X_test)\n",
        "        test_loss     = criterion(predictions, y_test.to(torch.long))\n",
        "        # .to(torch.int64)\n",
        "        _, predicted = torch.max(predictions, 1)  #returns max value, indices\n",
        "\n",
        "        total += y_test.size(0)\n",
        "        correct += (predicted == y_test).sum().item()  #.item() give the raw number\n",
        "        acc = 100 * (correct / total)\n",
        "    \n",
        "print(f\"Accuracy: {acc:2.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCssbguuhxZk",
        "outputId": "a72d1ad1-8123-4886-9675-0cdd7fab500b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZgHtH5ln6G0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}